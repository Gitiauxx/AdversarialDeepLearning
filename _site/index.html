<h1 id="adversarial-deep-learning">Adversarial Deep Learning</h1>
<p>Adversarial learning generates applies small perturbations to 
the inputs of a deep learning architecture so that the neural network
outputs a wrong answer with high confidence.</p>

<p><strong>Why does it matter?</strong>
The lack of robustness to small perturbation is seen as a sign that 
neural networks do not learn the true data manifold. Moreover, adversarial
attacker could use that weakness to generate visually undetectable 
input perturbations that lead to a misclassified outputs. Such attacks 
could have dramatic consequences when applied to system with safety or security 
concerns. Think of how problematic it would be if an attacker can paint stop signs so that 
deep learning algorithms deployed in autonomous vehicles would recognize them as
an other sign.</p>

<p><strong>What causes this vulnerability?</strong>
Initially, non-linearities were thought to explain the lack of robustness to adversarial 
inputs. However, Goodfellow et al. show that linear perturbations of non-linear models are 
sufficient to generate misclassification. Consider an image $\textbf{x}$ represented by integer
pixel value between 0 and 255. Any perturbation within 1/255 precision should not 
lead to different model output. Consider a perturbation $\eta$ within 1/255 precision and the
resuling perturbed input $\overline{x}$.
The linear combination of the perturbed image with a weight vector and $\overline{x}$, 
<script type="math/tex">w^{T}\overline{x} = w^{T}x + w^{T}\eta</script> is optimized by choosing $\eta=\epsilon sign(w)$. 
Therefore, a $\epsilon$ perturbation on each pixel of the image $x$ lead to
a total perturbation of magnitude $\epsilon m n$ after the linear combination, where 
$m$ is the average value of the weight $w$ across all dimensions and $n$ is the dimension
of $w$.</p>

<script type="math/tex; mode=display">mean = \frac{\displaystyle\sum_{i=1}^{n} x_{i}}{n}</script>

<p>the possibility
to</p>

<p>Adversarial examples are input to trick the neural network. 
Changes are imperceptible to human perception. 
Here attacks are white box attacks with access to model parameters.</p>

<ul>
  
</ul>
